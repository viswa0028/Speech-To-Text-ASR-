# -*- coding: utf-8 -*-
"""Speech processing assign1 rmse.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O6GTdn1a2dNA4kQZ25EDYHomaiX4BNhp
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from google.colab import drive
from pydub import AudioSegment
import gc

torch.cuda.empty_cache()
gc.collect()

!pip install pydub

root_dir = '/content/drive/MyDrive/dev-clean'

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

def convert_flac_to_wav(directory):
    flac_files = [f for f in os.listdir(directory) if f.endswith('.flac')]
    for flac_file in flac_files:
        wav_file = os.path.splitext(flac_file)[0] + '.wav'
        wav_path = os.path.join(directory, wav_file)

        if os.path.exists(wav_path):
            print(f"Skipping existing WAV: {wav_path}")
            continue
        flac_path = os.path.join(directory, flac_file)
        audio = AudioSegment.from_file(flac_path, format='flac')
        audio.export(wav_path, format='wav')
        print(f"Converted {flac_file} to {wav_file}")

def plot_spectrogram(Y, sr, hop_length, y_axis='linear', output_path=None):
    plt.figure(figsize=(2.56, 2.56))
    librosa.display.specshow(Y, hop_length=hop_length, sr=sr, y_axis=y_axis)
    plt.colorbar(format="%+2.0f")
    if output_path:
        plt.savefig(output_path, dpi=300)
        plt.close()

def feature_extraction(directory, audio_length=3.0, sample_rate=16000, n_mels=128, hop_size=512):
    wav_files = [f for f in os.listdir(directory) if f.endswith('.wav')]
    for wav_file in wav_files:
        wav_path = os.path.join(directory, wav_file)
        output_filename = os.path.splitext(wav_file)[0] + "_spectrogram.png"
        output_path = os.path.join(directory, output_filename)
        if os.path.exists(output_path):
            print(f"Skipping existing spectrogram: {output_path}")
            continue
        scale, sr = librosa.load(wav_path, sr=sample_rate)
        target_samples = int(audio_length * sr)
        if len(scale) < target_samples:
            scale = np.pad(scale, (0, target_samples - len(scale)), mode='constant')
        else:
            scale = scale[:target_samples]
        mel_spec = librosa.feature.melspectrogram(y=scale, sr=sr, n_mels=n_mels, hop_length=hop_size)
        Y_log_scale = librosa.power_to_db(mel_spec)
        plot_spectrogram(Y_log_scale, sr, hop_length=hop_size, y_axis='log', output_path=output_path)
        print(f"Saved spectrogram for {wav_file} to {output_path}")

def process_text_directory(directory):
    speaker_id = os.path.basename(os.path.dirname(directory))
    book_id = os.path.basename(directory)
    transcription_file = os.path.join(directory, f"{speaker_id}-{book_id}.trans.txt")
    if not os.path.exists(transcription_file):
        print(f"No transcription file {transcription_file}")
        return {}
    transcriptions = {}
    with open(transcription_file, 'r') as f:
        for line in f:
            if ' ' in line:
                filename, text = line.split(' ', 1)
                filename = filename.strip()
                text = text.strip().lower()
                transcriptions[filename] = text
    audio_files = [f for f in os.listdir(directory) if f.endswith('.wav')]
    audio_text_mapping = {}
    for audio_file in audio_files:
        filename_no_ext = os.path.splitext(audio_file)[0]
        if filename_no_ext in transcriptions:
            audio_text_mapping[audio_file] = transcriptions[filename_no_ext]
        else:
            print(f"Warning: No transcription for {audio_file} in {transcription_file}")
    output_file = os.path.join(directory, "cleaned_mapping.txt")
    with open(output_file, 'w') as f:
        for audio, text in audio_text_mapping.items():
            f.write(f"{audio} - {text}\n")
    print(f"Processed text for {directory}")
    return audio_text_mapping

for dirpath, _, filenames in os.walk(root_dir):
    if any(f.endswith('.flac') for f in filenames):
        print(f"Processing directory: {dirpath}")
        convert_flac_to_wav(dirpath)
        process_text_directory(dirpath)
        feature_extraction(dirpath)

def collect_image_files(root_dir):
    image_files = []
    for dirpath, _, filenames in os.walk(root_dir):
        for f in filenames:
            if f.endswith('.png'):
                image_files.append(os.path.join(dirpath, f))
    return image_files

image_files = collect_image_files(root_dir)
transforms1 = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor()
])
print(f"Found {len(image_files)} image files")

def collect_text_mappings(root_dir):
    text_mappings = {}
    for dirpath, _, filenames in os.walk(root_dir):
        if 'cleaned_mapping.txt' in filenames:
            mapping_file = os.path.join(dirpath, 'cleaned_mapping.txt')
            with open(mapping_file, 'r') as f:
                for line in f:
                    if ' - ' in line:
                        filenames, text = line.split(" - ", 1)
                        filenames = filenames.strip()
                        text = text.strip().lower()
                        png_name = filenames.replace('.wav', '_spectrogram.png')
                        rel_png_path = os.path.join(dirpath, png_name)
                        text_mappings[rel_png_path] = text
                    else:
                        print(f"Skipping malformed line in {mapping_file}: {line.strip()}")
    return text_mappings

text_mappings = collect_text_mappings(root_dir)
print(f"Text mappings for {len(text_mappings)} files")

def build_vocab(texts):
    chars = set(''.join(texts))
    char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(chars))}
    char_to_idx['<PAD>'] = 0
    char_to_idx['<SOS>'] = len(char_to_idx)
    char_to_idx['<EOS>'] = len(char_to_idx)
    idx_to_char = {idx: char for char, idx in char_to_idx.items()}
    return char_to_idx, idx_to_char

texts = list(text_mappings.values())
char_to_idx, idx_to_char = build_vocab(texts)
vocab_size = len(char_to_idx)

def text_to_indices(text, char_to_idx):
    return [char_to_idx['<SOS>']] + [char_to_idx[c] for c in text if c in char_to_idx] + [char_to_idx['<EOS>']]

y_train_dict = {filename: text_to_indices(text, char_to_idx) for filename, text in text_mappings.items()}
print(f"y_train_dict keys: {len(y_train_dict)}")

class SpeechDataset(Dataset):
    def __init__(self, image_files, y_train_dict, transform):
        self.image_files = image_files
        self.y_train_dict = y_train_dict
        self.transform = transform
        self.valid_pairs = []
        for i, filename in enumerate(self.image_files):
            if filename in y_train_dict:
                self.valid_pairs.append((i, filename))
            else:
                print(f'Filename {filename} not found')
        if len(self.valid_pairs) == 0:
            print("Error: No valid spectrogram-text pairs found")
        else:
            print(f"Created dataset with {len(self.valid_pairs)} valid pairs")

    def __len__(self):
        return len(self.valid_pairs)

    def __getitem__(self, idx):
        x_idx, filename = self.valid_pairs[idx]
        img = Image.open(filename).convert('L')
        spectrogram = self.transform(img)
        text_indices = self.y_train_dict[filename]
        return spectrogram, torch.tensor(text_indices, dtype=torch.long)

dataset = SpeechDataset(image_files, y_train_dict, transforms1)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2, collate_fn=lambda batch: (
    torch.stack([x for x, _ in batch]),
    torch.nn.utils.rnn.pad_sequence([y for _, y in batch], batch_first=True, padding_value=0)
))

class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        self.conv1 = nn.Conv2d(1, 256, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, stride=2)
        self.conv3 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)
        self.linear = nn.Linear(1024 * 32 * 32, 1024)

    def forward(self, x):
        x = F.relu(self.pool(self.conv1(x)))
        x = F.relu(self.pool(self.conv2(x)))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        features = F.relu(self.linear(x))
        hidden = features.unsqueeze(0)
        cell = torch.zeros_like(hidden)
        return features, (hidden, cell)

class Decoder(nn.Module):
    def __init__(self, vocab_size, embedding_size=300, hidden_size=1024):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        self.lstm1 = nn.LSTM(embedding_size + hidden_size, hidden_size, batch_first=True)
        self.lstm2 = nn.LSTM(hidden_size, hidden_size // 2, batch_first=True)
        self.linear = nn.Linear(hidden_size // 2, vocab_size)
        self.hidden_size = hidden_size

    def forward(self, x, encoder_features, hidden=None):
        batch_size, seq_len = x.size()
        x = self.embedding(x)
        encoder_features = encoder_features.unsqueeze(1).expand(-1, seq_len, -1)
        x = torch.cat([x, encoder_features], dim=2)
        if hidden is None:
            hidden = (torch.zeros(1, batch_size, self.hidden_size).to(x.device),
                      torch.zeros(1, batch_size, self.hidden_size).to(x.device))
        x, hidden1 = self.lstm1(x, hidden)
        x, hidden2 = self.lstm2(x)
        x = self.linear(x)
        return x, (hidden1, hidden2)

# Training
encoder = Encoder().to(device)
decoder = Decoder(vocab_size=vocab_size).to(device)
loss_function = nn.CrossEntropyLoss(ignore_index=0)
encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)
decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)

num_epochs = 10
accumulation_steps = 4  # Effective batch size = 4 * 4 = 16
for epoch in range(num_epochs):
    encoder.train()
    decoder.train()
    total_loss = 0
    batch_count = 0
    for batch_idx, (spectrogram, text) in enumerate(dataloader):
        spectrogram = spectrogram.to(device)
        text = text.to(device)

        encoder_optimizer.zero_grad(set_to_none=True)
        decoder_optimizer.zero_grad(set_to_none=True)

        encoder_features, encoder_hidden = encoder(spectrogram)
        decoder_input = text[:, :-1]
        decoder_target = text[:, 1:]
        decoder_output, _ = decoder(decoder_input, encoder_features, encoder_hidden)

        loss = loss_function(
            decoder_output.reshape(-1, vocab_size),
            decoder_target.reshape(-1)
        )
        loss = loss / accumulation_steps  # Scale loss
        loss.backward()

        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(dataloader):
            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0)
            torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1.0)
            encoder_optimizer.step()
            decoder_optimizer.step()
            torch.cuda.empty_cache()  # Clear memory after step

        total_loss += loss.item() * accumulation_steps
        batch_count += 1
        if (batch_idx + 1) % (50 * accumulation_steps) == 0:
            print(f"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item() * accumulation_steps:.4f}")

    avg_loss = total_loss / batch_count
    print(f"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}")

